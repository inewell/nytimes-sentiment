{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning for Text Project: Sentiment Analysis on Today's NYTimes\n",
    "\n",
    "### Isaac Newell\n",
    "\n",
    "This notebook pulls all the articles from today's New York Times (the webpage actually updates constantly). It retrieves the text from all of them and then processes that text. Here are the steps:\n",
    "1. Find all the \"a\" tags of articles from the today's paper homepage\n",
    "2. Use requests to get the content of these articles\n",
    "3. Split each article into sentences\n",
    "4. Perform sentiment analysis on each sentence\n",
    "5. Split these sentences into words and get the count for each word\n",
    "6. Output this processed data to a .json file\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll import import these two libraries:\n",
    "* BeautifulSoup, for parsing the DOM of webpages we load in\n",
    "* requests, for making HTTP requests to the NYTimes website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we address the real task, let's just test BeautifulSoup and requests on one article to retrieve its text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By JINA MOOREOCT. 27, 2017\n",
      "\n",
      "NAIROBI, Kenya — One month after a scathing United Nations report that called for a criminal investigation likely to lead back to its leaders, Burundi has withdrawn from the International Criminal Court, becoming the first country in the world to do so.\n",
      "A United Nations Commission of Inquiry on Burundi reported in September that it had found evidence of extrajudicial killings, disappearances, arbitrary arrests and detentions, torture and sexual violence in the two-and-a-half years since Burundi’s president, Pierre Nkurunziza, muscled his way to a third term in office.\n",
      "Burundi announced its intention to withdraw last year, at a time when the court was deeply unpopular with African leaders. Gambia and South Africa were also threatening to pull out, and the continent’s top intelligence officials signed a statement accusing the court of being “hijacked by powerful western countries” and “acting as a proxy” for foreign-led government change. But Mr. Nkurunziza can no longer count on leading a wave of exits.\n",
      "Gambia reversed course on its threat after Yahya Jammeh, its president for 22 years, departed following his electoral defeat by Adama Barrow. South Africa revoked its withdrawal in March, after its High Court ruled that Parliament would have to approve the measure.\n",
      "“For the court, I continue to think this is not going to mean very much,” said Kate Cronin-Furman, a human rights lawyer and postdoctoral fellow at the Harvard Kennedy School’s Belfer Center for Science and International Affairs. “Burundi is a pretty peripheral player, and their walkout is not going to threaten the court’s legitimacy or its prospects going forward.”\n",
      "Still, the court’s reputation took a hit last year when it essentially dropped charges against Kenya’s deputy president, William Ruto, for lack of evidence. Two years earlier, it withdrew charges against Uhuru Kenyatta, Kenya’s president. Both men had been accused of crimes against humanity over the violence after the country’s 2007 election, in which more than 1,100 people were killed and hundreds of thousands more were displaced.\n",
      "Nine of the 10 current formal investigations by the office of the court’s prosecutor concern Africa, and all of its trials so far have involved African defendants.\n",
      "“I think there is an ongoing concern about the court’s ability to work in countries in Africa,” said Rebecca Hamilton, an assistant professor of law at American University who previously worked in the International Criminal Court’s prosecution division.\n",
      "\n",
      "Please verify you're not a robot by clicking the box.\n",
      "Invalid email address. Please re-enter.\n",
      "You must select a newsletter to subscribe to.\n",
      "View all New York Times newsletters.\n",
      "“When you still have a government in power, we’re seeing how difficult it can be to conduct an investigation,” she added. “Where the court is interested in a rebel group or a group that’s not in power, it’s a different story. But power fights back against justice. That’s the bottom line, and that’s not unique to Africa.”\n",
      "Burundi might still end up in the court’s sights, however. Under its founding charter, known as the Rome Statute, crimes in nonmember states can still be referred for investigation by the United Nations Security Council. The United Nations’ commission of inquiry on Burundi recommended such a referral.\n",
      "The court’s work on events in Burundi has been in the exploratory stages, known as a “preliminary examination.” A spokesman for the court told The Associated Press that it would retain jurisdiction.\n",
      "It may struggle to do so, however, according to Mark Kersten, deputy director of the Wayamo Foundation, an international justice organization.\n",
      "“The rules are clearer on an investigation, but the I.C.C. hasn’t opened one,” he said. “It only has this preliminary examination.”\n",
      "For the examination to become an investigation, he said, the court’s judges would have to grant permission to the prosecutor, and Burundi might claim there was no legal basis for moving ahead after its withdrawal.\n",
      "“I think that could potentially create a lot of debate among other states,” Mr. Kersten said.\n",
      "A version of this article appears in print on October 28, 2017, on Page A4 of the New York edition with the headline: Trying to Sidestep a Potential Scandal, Burundi Quits International Criminal Court.  Order Reprints| Today's Paper|Subscribe\n",
      "\n",
      "\n",
      "We’re interested in your feedback on this page. Tell us what you think.\n",
      "Go to Home Page »\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(\"https://www.nytimes.com/2017/10/27/world/africa/burundi-international-criminal-court.html?rref=collection%2Fsectioncollection%2Fworld&action=click&contentCollection=world&region=stream&module=stream_unit&version=latest&contentPlacement=1&pgtype=sectionfront\")\n",
    "\n",
    "html_doc = r.text\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "ps = soup.find_all(\"p\")\n",
    "\n",
    "for p in ps:\n",
    "    tex = p.get_text()\n",
    "    if tex != \"Advertisement\":\n",
    "        print(tex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since loading that article worked properly, now we move on to the nytimes todayspaper page. That page has every article in the paper arranged by section and hyperlinked in. By inspecting the webpage structure, I figured  out where in the DOM all of the articles are. The front page is in a seperate div, so it had to be retrieved seperately from the other sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.nytimes.com/2017/11/18/nyregion/new-york-subway-system-failure-delays.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/18/us/politics/ron-johnson-senate-tax-cut.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/18/us/roy-moore-alabama.html?ref=todayspaper']\n",
      "['https://www.nytimes.com/2017/11/18/world/americas/rio-de-janeiro-brazil-violent-crime-security.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/18/world/middleeast/hariri-france-saudi-lebanon.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/18/world/americas/mexico-city-airport-enrique-pena-nieto.html?ref=todayspaper']\n",
      "['https://www.nytimes.com/2017/11/18/business/trump-wants-more-big-infrastructure-projects-the-obstacles-can-be-big-too.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/18/us/politics/republican-governors-trump-backlash-2018.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/18/nyregion/he-fled-myanmar-on-a-deathtrap-now-hes-the-luckiest-man-alive.html?ref=todayspaper']\n",
      "['https://www.nytimes.com/2017/11/17/obituaries/jeremy-hutchinson-a-top-lawyer-in-high-profile-cases-dies-at-102.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/17/obituaries/john-raines-84-who-evaded-capture-in-an-fbi-break-in-dies.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/18/fashion/azzedine-alaia-obituary.html?ref=todayspaper']\n",
      "['https://www.nytimes.com/2017/11/17/nyregion/judge-ruchie-the-hasidic-superwoman-of-night-court.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/16/nyregion/brooklyn-boat-to-haiti-stuffed-cars.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/17/nyregion/at-3700-a-month-affordable-apartments-go-begging.html?ref=todayspaper']\n",
      "['https://www.nytimes.com/2017/11/16/arts/music/john-adams-opera-san-francisco-girls.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/17/arts/television/alia-shawkat-search-party-transparent.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/17/arts/nutcracker-the-man-who-invented-christmas.html?ref=todayspaper']\n",
      "['https://www.nytimes.com/2017/11/18/business/a-hedge-fund-manager-committed-fraud-would-the-us-let-him-go.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/18/business/china-dating-schools.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/16/business/ryanair-pilots.html?ref=todayspaper']\n",
      "['https://www.nytimes.com/interactive/2017/11/17/opinion/sunday/What-if-You-Knew-Alzheimers-Was-Coming-for-You.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/17/opinion/sunday/escape-roy-moores-evangelicalism.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/17/opinion/pledges-fraternities-violence-deaths.html?ref=todayspaper']\n",
      "['https://www.nytimes.com/2017/11/13/travel/new-york-city-american-revolution.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/08/travel/vince-gill-favorite-museums.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/11/travel/cyber-monday-travel-deals.html?ref=todayspaper']\n",
      "['https://www.nytimes.com/2017/11/17/magazine/the-11-05-17-issue.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/15/magazine/is-loyalty-a-virtue-in-the-trump-era-its-complicated.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/17/magazine/new-sentences-from-duolingos-italian-lessons.html?ref=todayspaper']\n",
      "['https://www.nytimes.com/2017/11/13/books/review/the-kardashians-jerry-oppenheimer-raising-trump-ivana-trump.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/14/books/review/new-noteworthy-bari-weiss.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/17/books/review/letters-to-the-editor.html?ref=todayspaper']\n",
      "['https://www.nytimes.com/2017/11/19/sports/football/why-people-in-mississippi-have-to-watch-the-giants.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/17/sports/football/mexico-patriots-raiders.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/17/sports/ncaafootball/alabama-auburn-iron-bowl-sec.html?ref=todayspaper']\n",
      "['https://www.nytimes.com/2017/11/18/style/anne-wojcicki-23andme-genetics.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/18/style/doc-johnson-sex-toys.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/16/style/my-vagina-is-terrific-your-opinion-about-it-is-not.html?ref=todayspaper']\n",
      "['https://www.nytimes.com/2017/11/17/fashion/weddings/online-dating-match-married.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/15/fashion/weddings/tom-kirdahy-and-terrence-mcnally-an-immediate-and-lasting-need.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/19/fashion/weddings/tynishia-williams-thomas-powell-iii.html?ref=todayspaper']\n",
      "['https://www.nytimes.com/2017/11/17/realestate/putting-garden-to-bed.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/16/realestate/multigenerational-households-already-home-for-the-holidays.html?ref=todayspaper', 'https://www.nytimes.com/2017/11/18/realestate/what-to-do-when-a-neighbor-is-taking-your-newspaper.html?ref=todayspaper']\n"
     ]
    }
   ],
   "source": [
    "# Makes a dictionary with the urls of all articles in the current issue of the NYTimes\n",
    "# Output will contain a key for each section, whose value is a list of urls for all articles\n",
    "def get_url_dict():\n",
    "    r = requests.get(\"http://www.nytimes.com/pages/todayspaper/index.html\")\n",
    "\n",
    "    html_doc = r.text\n",
    "\n",
    "    soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "    \n",
    "    # Parse the DOM\n",
    "    main = soup.find(\"div\",attrs={\"id\":\"main\"})\n",
    "\n",
    "    front_page_div = main.find(\"div\",class_=\"aColumn\")\n",
    "    \n",
    "    # The frontpage is seperated into 2 seperate columns\n",
    "    fp_col1 = front_page_div.find(\"div\",class_=\"columnGroup first\")\n",
    "\n",
    "    fp_col2 = front_page_div.find(\"div\",class_=\"columnGroup singleRule last\")\n",
    "\n",
    "    col1_stories = fp_col1.findAll(\"div\",class_=\"story\")\n",
    "\n",
    "    # Create dictionary to store article urls in\n",
    "    urls = {}\n",
    "    \n",
    "    # Add the urls for the frontpage from both of the columns\n",
    "    urls[\"frontpage\"] = []\n",
    "    for story in col1_stories:\n",
    "        h3 = story.find(\"h3\")\n",
    "        a = h3.find(\"a\")\n",
    "        urls[\"frontpage\"].append(a.get(\"href\"))\n",
    "\n",
    "    col2_stories = fp_col2.findAll(\"a\")\n",
    "    for story in col2_stories:\n",
    "        urls[\"frontpage\"].append(story.get(\"href\"))\n",
    "    \n",
    "    # Now find the other sections, all stored in a seperate div\n",
    "    other_section_container = main.find(\"div\",attrs={\"id\":\"SpanABMiddleRegion\"})\n",
    "    secs = other_section_container.find_all(\"div\",class_=\"columnGroup\")\n",
    "    \n",
    "    # Iterate through the divs with class \"columnGroup\".\n",
    "    # Usually, every other one is an article, and\n",
    "    # every other other one is a \"jump to\" menu, not containing anything we want\n",
    "    for i,sec in enumerate(secs):\n",
    "        if i == len(secs)-1:\n",
    "            break\n",
    "        if len(sec.find_all(\"div\",class_=\"jumpToModule\")) == 0:\n",
    "            sec_name = sec.find(\"h3\",class_=\"sectionHeader\").find(\"a\").get(\"name\")\n",
    "            #print(sec_name)\n",
    "            urls[sec_name] = []\n",
    "            artic_list = sec.find(\"ul\").find_all(\"a\")\n",
    "            for artic in artic_list:\n",
    "                urls[sec_name].append(artic.get(\"href\"))\n",
    "    return urls\n",
    "\n",
    "# Now test this function\n",
    "urls = get_url_dict()\n",
    "# Print the first three urls in each section\n",
    "for k in urls.keys():\n",
    "    print(urls[k][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the urls for everything, we can make seperate HTTP requests for each one. From there we can get the title and text of each article. That is what this function does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_article_title_and_text(url):\n",
    "    r = requests.get(url)\n",
    "    html_doc = r.text\n",
    "    soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "    \n",
    "    # Extract all the <p> tags\n",
    "    ps = soup.find_all(\"p\",class_=\"story-body-text story-content\")\n",
    "    \n",
    "    # Attach the content of all of them to one big string\n",
    "    # Use BeautifulSoup's get_text method\n",
    "    # Replace curly quotes with straight quotes\n",
    "    # Curly quotes were messing up the sentence boundary finding (done later)\n",
    "    text = \"\"\n",
    "    for p in ps:\n",
    "         text = text+\" \"+p.get_text().replace('“','\"').replace('”','\"')\n",
    "    \n",
    "    # Get the article title, inside an <h1> with id=\"headline\"\n",
    "    title = soup.find_all(\"h1\",attrs={\"id\":\"headline\"})\n",
    "    if len(title) > 0:\n",
    "        title = title[0].get_text()\n",
    "    else:\n",
    "        title = \"\"\n",
    "    return {\"title\": title,\n",
    "            \"text\": text}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use get_article_title_and_text for all the urls. That is what this function does, and outputs it into a dictionary with a key for each section. The corresponding value is a list of articles, each represented by a dictionary with containing the title, url, and text content of each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_articles_dict():\n",
    "    urls = get_url_dict()\n",
    "    articles = {}\n",
    "    for k in urls.keys():\n",
    "        articles[k] = []\n",
    "        sec_urls = urls[k]\n",
    "        for url in sec_urls:\n",
    "            tt = get_article_title_and_text(url)\n",
    "            tt[\"url\"] = url\n",
    "            articles[k].append(tt)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frontpage How Politics and Bad Decisions Starved New York’s Subways  After a drumbeat of transit disasters this year, it became impossible to ignore the failures of the New York City subway system. A rush-hour Q train careened off the rails in southern Brooklyn. A tra\n",
      "\n",
      "world In Rio de Janeiro, ‘Complete Vulnerability’ as Violence Surges  RIO DE JANEIRO — For teachers in this seaside megacity, Rio de Janeiro’s surge in violence has meant making a life-or-death judgment call with unnerving frequency: deciding whether to cancel classes \n",
      "\n",
      "us Trump Wants More Big Infrastructure Projects. The Obstacles Can Be Big, Too.  President Trump says he is frustrated with the slow pace of major construction projects like highways, ports and pipelines. Last summer, he pledged to use the power of the presidency to jump start bu\n",
      "\n",
      "obituaries Jeremy Hutchinson, a Top Lawyer in High-Profile Cases, Dies at 102  LONDON — Jeremy Hutchinson, a British barrister whose sometimes theatrical courtroom tactics and rhetorical panache secured victories that helped reshape society’s attitudes toward obscenity, secrecy\n",
      "\n",
      "nyregion Judge Ruchie, the Hasidic Superwoman of Night Court  Just before the Jewish High Holy Days this fall, Judge Rachel Freier was rushing around her kitchen, as she perpetually is. She had just cooked a salmon dish for Sabbath dinner. She was talking to he\n",
      "\n",
      "arts John Adams Writes a New Opera, and It’s a Western  SIERRA CITY, Calif. — The fragrant firs had given way to jagged, rocky peaks, and the composer John Adams climbed a vertiginous metal staircase to a fire lookout high atop the Sierra Buttes, an aerie\n",
      "\n",
      "business A Hedge Fund Manager Committed Fraud. Would the U.S. Let Him Go?  TORONTO — Forty inmates lined up for the daily mail call at the Fort Dix Federal Correctional Institution, a complex of low-slung brick buildings in the middle of New Jersey. It was July 2012, and on\n",
      "\n",
      "sundayreview  \n",
      "\n",
      "travel When New York City Was a (Literal) Battlefield  New York City is a battlefield. I know what you’re thinking — psychological warfare, the endless grim clashing of economic forces — but I am being literal. When we ponder America’s defining war, the \n",
      "\n",
      "magazine The 11.05.17 Issue  RE: DEMOCRATS Robert Draper wrote about the Democratic Party’s attempts to navigate out of Barack Obama’s shadow. Why can’t they turn the page? Your article answered its own question. After a generat\n",
      "\n",
      "bookreview In ‘Raising Trump’ and ‘The Kardashians,’ Two Portraits of Modern American Matriarchy  RAISING TRUMPBy Ivana Trump 304 pp. Gallery Books. $26.99. THE KARDASHIANS An American Drama By Jerry Oppenheimer336 pp. St. Martin’s Press. $27.99. There are those who have fame thrust upon them, an\n",
      "\n",
      "sports Why People in Mississippi Have to Watch the Giants  Most football fans in the South who turn on CBS at 1 p.m. Sunday to watch N.F.L. football will see Baltimore at Green Bay. A much smaller portion of the audience in the South — mostly those living in\n",
      "\n",
      "styles The Doyenne of DNA Says: Just Chillax With Your Ex  MOUNTAIN VIEW, Calif. — In 2007, Anne Wojcicki, then 33, lassoed the moon. She was getting her new company, 23andMe, a mail-order genetics testing firm, off the ground with her \"Party ’til you spit\" \n",
      "\n",
      "weddings ‘Two Weird Stories’ Meet on Match.com  The sprawling home of Candice Marie Turner and Brian James Lee is so busy and lively, and the couple so welcoming, that there’s a sense a stranger could move in without anyone really noticing. At var\n",
      "\n",
      "realestate Time to Put the Garden to Bed?  There are 422 living trees for every human on Earth — 3.04 trillion overall — and during a couple of weeks each fall, a person can feel plainly outnumbered. Is it possible that a trillion of those tr\n",
      "\n"
     ]
    }
   ],
   "source": [
    "articles = get_all_articles_dict()\n",
    "for k in articles.keys():\n",
    "    first_article = articles[k][0]\n",
    "    title = first_article[\"title\"]\n",
    "    text = first_article[\"text\"]\n",
    "    print(k,title,text[:200])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we need to split the articles into sentences\n",
    "\n",
    "Splitting into sentences will allow us to perform sentiment analysis seperately on each sentence. Actually, sentence boundary disambiguation (SBD) is a more complicated task than it might sound (i.e. just finding periods), since abbreviations, question and exclamation marks, and quotes make the task more difficult. See the Wikipedia article on SBD for more.\n",
    "\n",
    "For this we will use the following the punkt sentence finder from nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.download_shell()\n",
    "# Uncomment this if you need to download the packages necessary for this.\n",
    "# I just downloaded everything, and then didn't need this line any more.\n",
    "# For some reason the regular nltk.download() wasn't working for me, but this did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test nltk.sent_tokenize on one sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My name is Isaac.',\n",
       " 'I live in a dorm at Andover called Stu.',\n",
       " 'Herbie also lives there.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"My name is Isaac. I live in a dorm at Andover called Stu. Herbie also lives there.\"\n",
    "\n",
    "nltk.sent_tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems to have worked well. Now we'll work with splitting the sentences into words and doing further analysis there. \n",
    "\n",
    "Initially I tried using a stemmer (which reduces a word to a root form, i.e. \"being\" goes to \"be\"). However the stemmer gets a lot of things wrong and makes a lot of fake words, i.e. \"flying\" goes to \"fli\". Lemmatizing is an alternative, which guarantees that the output is a real word. However, it really only works when you know the part of speech of the input, which is a difficult problem in itself. Thus I decided to abandon that idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wordcounts(wordlist):\n",
    "    counts = {}\n",
    "    for word in wordlist:\n",
    "        stem = word #stemmer.stem(word)\n",
    "        if stem in counts:\n",
    "            counts[stem] += 1\n",
    "        else:\n",
    "            counts[stem] = 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'be': 1, 'being': 1, 'not': 1, 'or': 1, 'to': 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl1 = [\"to\", \"be\", \"or\", \"not\", \"to\", \"being\"]\n",
    "get_wordcounts(wl1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use nltk's SentimentIntensityAnalyzer to perform sentiment analysis on all of our sentences. This is a pretrained model (from the VADER package, which stands for \"Valence Aware Dictionary and sEntiment Reasoner\", not Darth Vader). It outputs a vector of scores, each component between 0 and 1. The scores are positive, neutral, and negative. It also outputs a compound score, between -1 and 1. That is what we will use. This model is trained on social media, which could potentially have an inherent bias. Social media can be a nasty place, so when I actually finished this data visualization I noticed that many sentences that are clearly negative got classified as positive or neutral. It seems like anything that is not some nasty little message like a Trump tweet filled with words like \"sad\" and \"failing\" is biased towards positive. In any case, the algorithm isn't perfect and NLP is hard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaacnewell/anaconda/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the sentiment analyzer on a few sentences to make sure it's working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phillips Academy is a wonderful place.\n",
      "neg: 0.0, neu: 0.519, pos: 0.481, compound: 0.5719, \n",
      "Although Andover can work you half to death, overall its fun\n",
      "neg: 0.241, neu: 0.556, pos: 0.204, compound: -0.1531, \n",
      "I HATE Andover with a fiery passion!\n",
      "neg: 0.588, neu: 0.165, pos: 0.247, compound: -0.628, \n",
      "Trump can’t get his bad ideas through Congress, but he can use the power of the presidency to sabotage or even sink Obama’s signature deeds.\n",
      "neg: 0.229, neu: 0.771, pos: 0.0, compound: -0.7814, \n",
      "Our country is being ruined by Trump\n",
      "neg: 0.341, neu: 0.659, pos: 0.0, compound: -0.4767, \n",
      "Donald Trump is Making America Great Again!,\n",
      "neg: 0.0, neu: 0.577, pos: 0.423, compound: 0.6588, \n",
      "We faked the moon landing\n",
      "neg: 0.0, neu: 1.0, pos: 0.0, compound: 0.0, \n"
     ]
    }
   ],
   "source": [
    "test_sents = [\"Phillips Academy is a wonderful place.\",\n",
    "             \"Although Andover can work you half to death, overall its fun\",\n",
    "             \"I HATE Andover with a fiery passion!\",\n",
    "             \"Trump can’t get his bad ideas through Congress, but he can use the power of the presidency to sabotage or even sink Obama’s signature deeds.\",\n",
    "             \"Our country is being ruined by Trump\",\n",
    "             \"Donald Trump is Making America Great Again!,\",\n",
    "             \"We faked the moon landing\"]\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "for sent in test_sents:\n",
    "    print(sent)\n",
    "    ss = sia.polarity_scores(sent)\n",
    "    for k in ss:\n",
    "        print(\"{0}: {1}, \".format(k, ss[k]), end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! It seems to work decently well, and classifies the varying sentences about Andover correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to keep track of the counts of words and their presence or absence in a given sentence for our later analysis. We will use the Counter to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we want to filter the words a bit. We'll use nltk's stopwords list, which contains common, mostly insignificant words like \"i\",\"your\", and other pronouns, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him'] 15\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english')[:15], len(stopwords.words('english')[:15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also filter out punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checks if a word is valid with the three criteria:\n",
    "# 1. If it is not in the stopwords\n",
    "# 2. If it is at least 4 characters long\n",
    "# 3. If it doesn't start with punctuation\n",
    "def is_valid(word):\n",
    "    if word in stopwords.words('english'):\n",
    "        return False;\n",
    "    if len(word) < 4:\n",
    "        return False;\n",
    "    if word[0] in string.punctuation:\n",
    "        return False;\n",
    "    return True;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to finish processing and store our text data. We will store it in a dictionary. That dictionary will have a key for every section, which will correspond to a list of articles, each represented by a dictionary. Each article has a title, a url, and its content. Its content is a list of dictionaries, one corresponding to each sentence within the article. Those dictionaries contain the list of split significant words and their counts, the raw text of the sentence, and the calculated sentiment. We will also save a seperate data structure that counts words globally throughout a selection of sections (frontpage, world, us, opinion, nyregion, business, sundayreview). These data structures will both be output into seperate JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "dictionary = Counter()\n",
    "\n",
    "for k in articles.keys():\n",
    "    sec = articles[k]\n",
    "    sec_content = []\n",
    "    for artic in sec:\n",
    "        artic_content = []\n",
    "        artic_sentences = nltk.sent_tokenize(artic[\"text\"])\n",
    "         \n",
    "        for sent in artic_sentences:\n",
    "            sent_obj = {}\n",
    "            sent_obj[\"sentence\"] = sent\n",
    "            sent_words = nltk.word_tokenize(sent)\n",
    "            sent_obj[\"words\"] = get_wordcounts(sent_words)\n",
    "            if k in [\"frontpage\",\"world\",\"us\",\"opinion\",\"nyregion\",\"business\",\"sundayreview\"]:\n",
    "                useful_words = [w for w in sent_words if is_valid(w)]\n",
    "                dictionary.update(useful_words)\n",
    "            sent_obj[\"sentiment\"] = sia.polarity_scores(sent)[\"compound\"]\n",
    "            artic_content.append(sent_obj)\n",
    "        sec_content.append({\"title\": artic[\"title\"],\n",
    "                            \"url\": artic[\"url\"],\n",
    "                           \"content\": artic_content})\n",
    "    data[k] = sec_content       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to write the data to a JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"nytimes_sentiment.json\", \"w\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we will output the dictionary of wordcounts to a JSON file. Some selection of the top words will be displayed on the final visualization.\n",
    "\n",
    "Let's just print out the 20 most common words, to make sure it's working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('said', 677), ('would', 257), ('people', 176), ('years', 162), ('Baker', 157), ('could', 145), ('year', 142), ('like', 139), ('time', 130), ('state', 124), ('They', 118), ('also', 112), ('government', 108), ('first', 107), ('work', 101), ('family', 99), ('many', 93), ('percent', 90), ('home', 89), ('back', 87)]\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note: interestingly, \"said\" is always the number 1 word, by far, every day that I've run this script. It makes sense, since the NYTimes is a newspaper, which means that it frequently quotes people. If you look at just the opinion section, for example, however, there are almost no quotes and the top word is almost always \"Trump\". It's also interesting how this list of words is often so variable over time based off of current stories, i.e. \"Weinstein\" has recently been consistently in the top words because that story is big news currently. Some words, like \"Trump\" are more stably at the top.\n",
    "\n",
    "Now output that into a JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"top_words.json\", \"w\") as f:\n",
    "    json.dump({\"counts\": dictionary.most_common(1000)}, f, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
